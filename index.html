<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.7"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>TinyChatEngine: TinyChatEngine: A Efficient Neural Network Library for LLM</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">TinyChatEngine
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.7 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">TinyChatEngine: A Efficient Neural Network Library for LLM </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="md_README"></a> TinyChatEngine is a powerful neural network library specifically designed for the efficient deployment of quantized large language models (LLMs) on edge devices.</p>
<p><img src="assets/figures/chat.gif" alt="demo" class="inline"/></p>
<h1><a class="anchor" id="autotoc_md1"></a>
Prerequisites</h1>
<h2><a class="anchor" id="autotoc_md2"></a>
MacOS</h2>
<p>For MacOS, install boost and llvm by</p>
<div class="fragment"><div class="line">brew install boost</div>
<div class="line">brew install llvm</div>
</div><!-- fragment --><p>For M1/M2 users, install Xcode from AppStore to enable the metal compiler for GPU support.</p>
<h2><a class="anchor" id="autotoc_md3"></a>
Windows</h2>
<p>For Windows, download and install the GCC compiler with MSYS2. Follow this tutorial: <a href="https://code.visualstudio.com/docs/cpp/config-mingw">https://code.visualstudio.com/docs/cpp/config-mingw</a> for installation.</p>
<ul>
<li>Install required dependencies with MSYS2</li>
</ul>
<div class="fragment"><div class="line">pacman -S --needed base-devel mingw-w64-x86_64-toolchain make unzip git</div>
</div><!-- fragment --><ul>
<li>Add binary directories (e.g., C:\msys64\mingw64\bin and C:\msys64\usr\bin) to the environment path</li>
</ul>
<h2><a class="anchor" id="autotoc_md4"></a>
Kernel support list</h2>
<p>| Kernel precision | x86 (Intel/AMD CPU) | ARM (Apple M1/M2) | Nvidia GPU | Apple GPU | | ---&mdash; | ------------------------&mdash; | ------&mdash; | ------&mdash; | ------&mdash; | | FP16/FP32 | ✅ | ✅ | | | W4A16 | | | ✅ | ✅ | W4A32 | ✅ | ✅ | | ✅ | W4A8 | ✅ | ✅ | | | W8A8 | ✅ | ✅ | |</p>
<h2><a class="anchor" id="autotoc_md5"></a>
Model and quantization method support list</h2>
<p>| Models | AWQ (INT4) | SmoothQuant (INT8) | | ---&mdash; | ------&mdash; | ------&mdash; | | LLaMA-2 | ✅ | | LLaMA | ✅ | | Vicuna | ✅ | | OPT | ✅ | ✅ | MPT | | | Falcon | |</p>
<h1><a class="anchor" id="autotoc_md6"></a>
Quantization and Model Support</h1>
<p>The goal of TinyChatEngine is to support various quantization methods on various devices. For example, At present, it supports the quantized weights for int8 OPT models that originate from <a href="https://github.com/mit-han-lab/smoothquant">smoothquant</a> and can be converted to TinyChatEngine format using the provided conversion script <a href="transformer/opt_smooth_exporter.py">opt_smooth_exporter.py</a>. For LLaMA models, scripts are available for converting Huggingface format checkpoints to our <a href="transformer/llama_exporter.py">format</a>, and for quantizing them to specific methods <a href="transformer/model_quantizer.py">based on your device</a>. We also plan to more models.</p>
<h2><a class="anchor" id="autotoc_md7"></a>
Device-specific Quantization Methods</h2>
<p>Different target devices require different quantization methods due to the variants of kernel implementation that suit the SIMD bit-width and instructions supported by your device. To quantize your LLaMA model to int4, please consult the following table:</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Platforms   </th><th class="markdownTableHeadNone">ISA   </th><th class="markdownTableHeadNone">Quantization methods    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Intel/AMD   </td><td class="markdownTableBodyNone">x86-64   </td><td class="markdownTableBodyNone">QM_x86    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">M1/M2 Mac   </td><td class="markdownTableBodyNone">arm   </td><td class="markdownTableBodyNone">QM_ARM   </td></tr>
</table>
<p>Example of quantizing a LLaMA model for an Intel/AMD laptop:</p>
<div class="fragment"><div class="line">python model_quantizer.py --model_path models/LLaMA_7B --method QM_x86 --output_path INT4/</div>
</div><!-- fragment --><p>Example of quantizing a LLaMA model for an M1/M2 Macbook:</p>
<div class="fragment"><div class="line">python model_quantizer.py --model_path models/LLaMA_7B --method QM_ARM --output_path INT4/</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md8"></a>
Download and deploy models from our Model Zoo</h2>
<p>We offer a selection of models that have been tested with TinyChatEngine. These models can be readily downloaded and deployed on your device. To download a model, locate the target model's ID in the table below and use the associated script.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Models   </th><th class="markdownTableHeadNone">Size   </th><th class="markdownTableHeadNone">ID   </th><th class="markdownTableHeadNone">Supported Precision    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">LLaMA-2   </td><td class="markdownTableBodyNone">7B/13B   </td><td class="markdownTableBodyNone">LLaMA_7B_2_chat/LLaMA_13B_2_chat   </td><td class="markdownTableBodyNone">INT4/FP32    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">OPT   </td><td class="markdownTableBodyNone">125m/1.3B/6.7B   </td><td class="markdownTableBodyNone">OPT_125/OPT_1.3B/OPT_6.7B   </td><td class="markdownTableBodyNone">INT4/INT8/FP32   </td></tr>
</table>
<p>For instance, to download the quantized LLaMA-2-7B-chat model:</p>
<ul>
<li>On a Intel/AMD latptop: <div class="fragment"><div class="line">python download_model.py --model LLaMA_7B_2_chat --QM QM_x86</div>
</div><!-- fragment --></li>
<li>On a M1/M2 Macbook: <div class="fragment"><div class="line">python download_model.py --model LLaMA_7B_2_chat --QM QM_ARM</div>
</div><!-- fragment --></li>
</ul>
<p>To deploy the quantized model with TinyChatEngine, compile the chat program and run it with the model ID and precision.</p>
<div class="fragment"><div class="line">make chat -j</div>
<div class="line">./chat LLaMA_7B_2_chat INT4</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md9"></a>
Step-by-step to deploy LLaMA2-7B-chat with TinyChatEngine</h1>
<p>Here, we provide step-by-step instructions to deploy LLaMA2-7B-chat with TinyChatEngine from scratch.</p>
<ul>
<li>Download the repo. <div class="fragment"><div class="line"># pull repo</div>
<div class="line">git clone --recursive https://github.com/mit-han-lab/TinyChatEngine.git</div>
</div><!-- fragment --></li>
<li>Download the quantized LLaMA2-7B-chat model from our model zoo. <div class="fragment"><div class="line">cd TinyChatEngine/transformer</div>
</div><!-- fragment --><ul>
<li>On a x86 device (e.g., Intel/AMD laptop) <div class="fragment"><div class="line">python download_model.py --model LLaMA_7B_2_chat --QM QM_x86</div>
</div><!-- fragment --></li>
<li>On a ARM device (e.g., M1/M2 Macbook) <div class="fragment"><div class="line">python download_model.py --model LLaMA_7B_2_chat --QM QM_ARM</div>
</div><!-- fragment --></li>
<li>On a CUDA device (e.g., Jetson AGX Orin) ```bash python download_model.py &ndash;model LLaMA_7B_2_chat &ndash;QM QM_CUDA <div class="fragment"><div class="line">- Compile and start the chat locally.</div>
</div><!-- fragment --> bash make chat -j ./chat # chat.exe on Windows Using model: LLaMA7B_2_chat Using LLaMA's default data format: INT4 Loading model... Finished! USER: Write a syllabus for Operating Systems. ASSISTANT: Of course! Here is a sample syllabus for a college-level course on operating systems: Course Title: Introduction to Operating Systems Course Description: This course provides an overview of the fundamental concepts and techniques used in modern operating systems, including process management, memory management, file systems, security, and I/O devices. Students will learn how these components work together to provide a platform for running applications and programs on a computer. Course Objectives:</li>
<li>Understand the basic architecture of an operating system</li>
<li>Learn about processes, threads, and process scheduling algorithms</li>
<li>Study memory management techniques such as paging and segmentation</li>
<li>Explore file systems including file organization, storage devices, and file access methods</li>
<li>Investigate security mechanisms to protect against malicious software attacks</li>
<li>Analyze input/output (I/O) operations and their handling by the operating system ... <div class="fragment"><div class="line">## Instructions to run a speech-to-speech chatbot demo</div>
<div class="line"> </div>
<div class="line">- Follow instructions above to deploy LLaMA2-7B-chat</div>
<div class="line"> </div>
<div class="line">- Configure whisper.cpp (Note)</div>
</div><!-- fragment --> bash cd whisper.cpp</li>
</ul>
</li>
</ul>
<h1><a class="anchor" id="autotoc_md10"></a>
Install SDL2 on Linux</h1>
<p>sudo apt-get install libsdl2-dev </p>
<h1><a class="anchor" id="autotoc_md11"></a>
Install SDL2 on Mac OS</h1>
<p>brew install sdl2</p>
<p>git apply ./../clean_up_patch bash ./models/download-ggml-model.sh base.en </p>
<h1><a class="anchor" id="autotoc_md12"></a>
NVIDIA GPU (Note: you may need to change the Makefile of whisper.cpp depending on your environment or device)</h1>
<p>WHISPER_CUBLAS=1 make -j stream </p>
<h1><a class="anchor" id="autotoc_md13"></a>
Otherwise</h1>
<p>make stream cd ../ </p><div class="fragment"><div class="line">- If you have an edge device and want a better TTS program than espeak, download [piper](https://github.com/rhasspy/piper)</div>
</div><!-- fragment --><p> bash mkdir TTS wget <a href="https://github.com/rhasspy/piper/releases/download/v1.2.0/piper_arm64.tar.gz">https://github.com/rhasspy/piper/releases/download/v1.2.0/piper_arm64.tar.gz</a> tar -xvzf piper_arm64.tar.gz </p><div class="fragment"><div class="line">  - Download your preferred voice from the [huggingface repo](https://huggingface.co/rhasspy/piper-voices/tree/v1.0.0) and drag both the .onxx and .onnx.json files into the TTS directory</div>
<div class="line"> </div>
<div class="line">- Edit the listen shell file in the transformers directory so whisper.cpp is using your preferred parameters.</div>
</div><!-- fragment --><p> bash nano listen </p><div class="fragment"><div class="line">- Edit the speak shell file in the transformers directory so the demo uses your preferred TTS program.</div>
</div><!-- fragment --><p> bash nano speak </p><div class="fragment"><div class="line">- Compile and start the voicechat locally.</div>
</div><!-- fragment --><p> bash make -j voicechat ./voicechat # voicechat.exe on Windows ```</p>
<h2><a class="anchor" id="autotoc_md14"></a>
Related Projects</h2>
<p><a href="https://github.com/mit-han-lab/tinyengine">TinyEngine</a></p>
<p><a href="https://github.com/mit-han-lab/smoothquant">Smoothquant</a></p>
<p><a href="https://github.com/mit-han-lab/llm-awq">AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</a></p>
<h2><a class="anchor" id="autotoc_md15"></a>
Acknowledgement</h2>
<p><a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a></p>
<p><a href="https://github.com/huggingface/transformers">transformers</a> </p>
</div></div><!-- PageDoc -->
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.7
</small></address>
</body>
</html>
