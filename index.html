<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.7"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>TinyChatEngine: TinyChatEngine: A Efficient Neural Network Library for LLM</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">TinyChatEngine
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.7 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">TinyChatEngine: A Efficient Neural Network Library for LLM </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="md_README"></a> TinyChatEngine is a powerful neural network library specifically designed for the efficient deployment of quantized large language models (LLMs) on edge devices.</p>
<p><img src="assets/figures/chat.gif" alt="demo" class="inline"/></p>
<h1><a class="anchor" id="autotoc_md1"></a>
Prerequisites</h1>
<h2><a class="anchor" id="autotoc_md2"></a>
MacOS</h2>
<p>For MacOS, install boost and llvm by</p>
<div class="fragment"><div class="line">brew install boost</div>
<div class="line">brew install llvm</div>
</div><!-- fragment --><p>For M1/M2 users, install Xcode from AppStore to enable the metal compiler for GPU support.</p>
<h2><a class="anchor" id="autotoc_md3"></a>
Windows</h2>
<p>For Windows, download and install the GCC compiler with MSYS2. Follow this tutorial: <a href="https://code.visualstudio.com/docs/cpp/config-mingw">https://code.visualstudio.com/docs/cpp/config-mingw</a> for installation.</p>
<ul>
<li>Install required dependencies with MSYS2</li>
</ul>
<div class="fragment"><div class="line">pacman -S --needed base-devel mingw-w64-x86_64-toolchain make unzip git</div>
</div><!-- fragment --><ul>
<li>Add binary directories (e.g., C:\msys64\mingw64\bin and C:\msys64\usr\bin) to the environment path</li>
</ul>
<h1><a class="anchor" id="autotoc_md4"></a>
Step-by-step to deploy LLaMA2-7B-chat with TinyChatEngine</h1>
<p>Here, we provide step-by-step instructions to deploy LLaMA2-7B-chat with TinyChatEngine from scratch.</p>
<ul>
<li>Download the repo. <div class="fragment"><div class="line">git clone --recursive git@github.com:mit-han-lab/TinyChatEngine.git</div>
</div><!-- fragment --></li>
<li><p class="startli">Download the quantized LLaMA2-7B-chat model from our model zoo. </p><div class="fragment"><div class="line">cd TinyChatEngine/llm</div>
</div><!-- fragment --><ul>
<li>On an x86 device (e.g., Intel/AMD laptop) <div class="fragment"><div class="line">python tools/download_model.py --model LLaMA2_7B_chat_awq_int4 --QM QM_x86</div>
</div><!-- fragment --></li>
<li>On an ARM device (e.g., M1/M2 Macbook) <div class="fragment"><div class="line">python tools/download_model.py --model LLaMA2_7B_chat_awq_int4 --QM QM_ARM</div>
</div><!-- fragment --></li>
<li>On a CUDA device (e.g., Jetson AGX Orin) ```bash python tools/download_model.py &ndash;model LLaMA2_7B_chat_awq_int4 &ndash;QM QM_CUDA <div class="fragment"><div class="line">- Compile and start the chat locally.</div>
</div><!-- fragment --> bash make chat -j ./chat Using model: LLaMA7B_2_chat Using LLaMA's default data format: INT4 Loading model... Finished! USER: Write a syllabus for Operating Systems. ASSISTANT: Of course! Here is a sample syllabus for a college-level course on operating systems: Course Title: Introduction to Operating Systems Course Description: This course provides an overview of the fundamental concepts and techniques used in modern operating systems, including process management, memory management, file systems, security, and I/O devices. Students will learn how these components work together to provide a platform for running applications and programs on a computer. Course Objectives:</li>
<li>Understand the basic architecture of an operating system</li>
<li>Learn about processes, threads, and process scheduling algorithms</li>
<li>Study memory management techniques such as paging and segmentation</li>
<li>Explore file systems including file organization, storage devices, and file access methods</li>
<li>Investigate security mechanisms to protect against malicious software attacks</li>
<li>Analyze input/output (I/O) operations and their handling by the operating system ...</li>
</ul>
<div class="fragment"><div class="line">## Kernel support</div>
<div class="line"> </div>
<div class="line">| Kernel precision | x86 (Intel/AMD CPU) | ARM (Apple M1/M2) | Nvidia GPU | Apple GPU |</div>
<div class="line">| ------ | --------------------------- | --------- | --------- | --------- |</div>
<div class="line">| FP32   |  ✅    |    ✅  |         |</div>
<div class="line">| FP16   |     |      |         |</div>
<div class="line">| W4A16  |      |      |  ✅  | ✅</div>
<div class="line">| W4A32  |  ✅  |  ✅  |      | ✅</div>
<div class="line">| W4A8   |  ✅  |  ✅  |      |</div>
<div class="line">| W8A8   |  ✅  |  ✅  |      |</div>
<div class="line"> </div>
<div class="line">## Quantization and Model Support</div>
<div class="line"> </div>
<div class="line">The goal of TinyChatEngine is to support various quantization methods on various devices. For example, At present, it supports the quantized weights for int8 opt models that originate from [smoothquant](https://github.com/mit-han-lab/smoothquant) using the provided conversion script [opt_smooth_exporter.py](llm/opt_smooth_exporter.py). For LLaMA models, scripts are available for converting Huggingface format checkpoints to our int4 wegiht [format](llm/llama_exporter.py), and for quantizing them to specific methods [based on your device](llm/model_quantizer.py). Before converting and quantizing your models, it is recommended to apply the fake quantization from [AWQ](https://github.com/mit-han-lab/llm-awq) to achieve better accuracy. We are currently working on supporting more models, please stay tuned!</div>
<div class="line"> </div>
<div class="line">### Device-specific int4 Weight Reordering</div>
<div class="line"> </div>
<div class="line">To mitigate the runtime overheads associated with weight reordering, TinyChatEngine conducts this process offline during model conversion. In this section, we will explore the weight layouts of QM_ARM and QM_x86. These layouts are tailored for ARM and x86 CPUs, supporting 128-bit SIMD and 256-bit SIMD operations, respectively. We also support QM_CUDA for Nvidia GPUs, including server and edge GPUs.</div>
<div class="line"> </div>
<div class="line">| Platforms  | ISA | Quantization methods |</div>
<div class="line">| ------------- | ------------- |  ------------- |</div>
<div class="line">| Intel/AMD |  x86-64  | QM_x86  |</div>
<div class="line">| Apple M1/M2 Mac | arm | QM_ARM  |</div>
<div class="line">| Nvidia GPU| CUDA | QM_CUDA  |</div>
<div class="line"> </div>
<div class="line">- Example layout of QM_ARM: For QM_ARM, consider the initial configuration of a 128-bit weight vector, \[w0, w1, ... , w30, w31\], where each wi is a 4-bit quantized weight. TinyChatEngine rearranges these weights in the sequence  \[w0, w16, w1, w17, ..., w15, w31\] by interleaving the lower half and upper half of the weights. This new arrangement facilitates the decoding of both the lower and upper halves using 128-bit AND and shift operations, as depicted in the subsequent figure. This will eliminate runtime reordering overheads and improve performance.</div>
<div class="line"> </div>
<div class="line">### Download and deploy models from our Model Zoo</div>
<div class="line"> </div>
<div class="line">We offer a selection of models that have been tested with TinyChatEngine. These models can be readily downloaded and deployed on your device. To download a model, locate the target model&#39;s ID in the table below and use the associated script.</div>
<div class="line"> </div>
<div class="line">&lt;table&gt;</div>
<div class="line">    &lt;thead&gt;</div>
<div class="line">        &lt;tr&gt;</div>
<div class="line">            &lt;th&gt;Models&lt;/th&gt;</div>
<div class="line">            &lt;th&gt;Precisions&lt;/th&gt;</div>
<div class="line">            &lt;th&gt;ID&lt;/th&gt;</div>
<div class="line">        &lt;/tr&gt;</div>
<div class="line">    &lt;/thead&gt;</div>
<div class="line">    &lt;tbody&gt;</div>
<div class="line">        &lt;tr&gt;</div>
<div class="line">            &lt;td rowspan=&quot;2&quot;&gt;LLaMA2_13B_chat&lt;/td&gt;</div>
<div class="line">            &lt;td&gt; fp32&lt;/td&gt;</div>
<div class="line">            &lt;td&gt; LLaMA2_13B_chat_fp32 &lt;/td&gt;</div>
<div class="line">        &lt;/tr&gt;</div>
<div class="line">        &lt;tr&gt;</div>
<div class="line">            &lt;!-- No data for the first column here because it&#39;s merged with data1 --&gt;</div>
<div class="line">            &lt;td&gt;int4&lt;/td&gt;</div>
<div class="line">            &lt;td&gt;LLaMA2_13B_chat_awq_int4&lt;/td&gt;</div>
<div class="line">        &lt;/tr&gt;</div>
<div class="line">        &lt;tr&gt;</div>
<div class="line">            &lt;td rowspan=&quot;2&quot;&gt;LLaMA2_7B_chat&lt;/td&gt;</div>
<div class="line">            &lt;td&gt;fp32&lt;/td&gt;</div>
<div class="line">            &lt;td&gt;LLaMA2_7B_chat_fp32 &lt;/td&gt;</div>
<div class="line">        &lt;/tr&gt;</div>
<div class="line">        &lt;tr&gt;</div>
<div class="line">            &lt;!-- No data for the first column here because it&#39;s merged with data1 --&gt;</div>
<div class="line">            &lt;td&gt; int4&lt;/td&gt;</div>
<div class="line">            &lt;td&gt; LLaMA2_7B_chat_awq_int4&lt;/td&gt;</div>
<div class="line">        &lt;/tr&gt;</div>
<div class="line">        &lt;tr&gt;</div>
<div class="line">            &lt;td rowspan=&quot;2&quot;&gt;LLaMA_7B&lt;/td&gt;</div>
<div class="line">            &lt;td&gt; fp32&lt;/td&gt;</div>
<div class="line">            &lt;td&gt; LLaMA_7B_fp32 &lt;/td&gt;</div>
<div class="line">        &lt;/tr&gt;</div>
<div class="line">        &lt;tr&gt;</div>
<div class="line">            &lt;!-- No data for the first column here because it&#39;s merged with data1 --&gt;</div>
<div class="line">            &lt;td&gt;int4&lt;/td&gt;</div>
<div class="line">            &lt;td&gt;LLaMA_7B_awq_int4&lt;/td&gt;</div>
<div class="line">        &lt;/tr&gt;</div>
<div class="line">        &lt;tr&gt;</div>
<div class="line">            &lt;td rowspan=&quot;3&quot;&gt;opt-6.7B&lt;/td&gt;</div>
<div class="line">            &lt;td&gt;fp32&lt;/td&gt;</div>
<div class="line">            &lt;td&gt;opt_6.7B_fp32&lt;/td&gt;</div>
<div class="line">        &lt;/tr&gt;</div>
<div class="line">        &lt;tr&gt;</div>
<div class="line">            &lt;!-- No data for the first column here because it&#39;s merged with data1 --&gt;</div>
<div class="line">            &lt;td&gt;int8&lt;/td&gt;</div>
<div class="line">            &lt;td&gt;opt_6.7B_smooth_int8&lt;/td&gt;</div>
<div class="line">        &lt;/tr&gt;</div>
<div class="line">        &lt;tr&gt;</div>
<div class="line">            &lt;!-- No data for the first column here because it&#39;s merged with data1 --&gt;</div>
<div class="line">            &lt;td&gt; int4&lt;/td&gt;</div>
<div class="line">            &lt;td&gt; opt_6.7B_awq_int4&lt;/td&gt;</div>
<div class="line">        &lt;/tr&gt;</div>
<div class="line">        &lt;tr&gt;</div>
<div class="line">            &lt;td rowspan=&quot;3&quot;&gt;opt-1.3B&lt;/td&gt;</div>
<div class="line">            &lt;td&gt;fp32&lt;/td&gt;</div>
<div class="line">            &lt;td&gt;opt_1.3B_fp32&lt;/td&gt;</div>
<div class="line">        &lt;/tr&gt;</div>
<div class="line">        &lt;tr&gt;</div>
<div class="line">            &lt;!-- No data for the first column here because it&#39;s merged with data1 --&gt;</div>
<div class="line">            &lt;td&gt;int8&lt;/td&gt;</div>
<div class="line">            &lt;td&gt;opt_1.3B_smooth_int8&lt;/td&gt;</div>
<div class="line">        &lt;/tr&gt;</div>
<div class="line">        &lt;tr&gt;</div>
<div class="line">            &lt;!-- No data for the first column here because it&#39;s merged with data1 --&gt;</div>
<div class="line">            &lt;td&gt; int4&lt;/td&gt;</div>
<div class="line">            &lt;td&gt; opt_1.3B_awq_int4&lt;/td&gt;</div>
<div class="line">        &lt;/tr&gt;</div>
<div class="line">        &lt;tr&gt;</div>
<div class="line">            &lt;td rowspan=&quot;3&quot;&gt;opt-125m&lt;/td&gt;</div>
<div class="line">            &lt;td&gt;fp32&lt;/td&gt;</div>
<div class="line">            &lt;td&gt;opt_125m_fp32&lt;/td&gt;</div>
<div class="line">        &lt;/tr&gt;</div>
<div class="line">        &lt;tr&gt;</div>
<div class="line">            &lt;!-- No data for the first column here because it&#39;s merged with data1 --&gt;</div>
<div class="line">            &lt;td&gt;int8&lt;/td&gt;</div>
<div class="line">            &lt;td&gt;opt_125m_smooth_int8&lt;/td&gt;</div>
<div class="line">        &lt;/tr&gt;</div>
<div class="line">        &lt;tr&gt;</div>
<div class="line">            &lt;!-- No data for the first column here because it&#39;s merged with data1 --&gt;</div>
<div class="line">            &lt;td&gt; int4&lt;/td&gt;</div>
<div class="line">            &lt;td&gt; opt_125m_awq_int4&lt;/td&gt;</div>
<div class="line">        &lt;/tr&gt;</div>
<div class="line">    &lt;/tbody&gt;</div>
<div class="line">&lt;/table&gt;</div>
<div class="line"> </div>
<div class="line">For instance, to download the quantized LLaMA-2-7B-chat model: (for int4 models, use --QM  to choose the quantized model for your device)</div>
<div class="line"> </div>
<div class="line">- On an Intel/AMD latptop:</div>
</div><!-- fragment --><p> bash python tools/download_model.py &ndash;model LLaMA2_7B_chat_awq_int4 &ndash;QM QM_x86 </p><div class="fragment"><div class="line">- On an M1/M2 Macbook:</div>
</div><!-- fragment --><p> bash python tools/download_model.py &ndash;model LLaMA2_7B_chat_awq_int4 &ndash;QM QM_ARM </p><div class="fragment"><div class="line">- On an Nvidia GPU:</div>
</div><!-- fragment --><p> bash python tools/download_model.py &ndash;model LLaMA2_7B_chat_awq_int4 &ndash;QM QM_CUDA </p><div class="fragment"><div class="line">To deploy a quantized model with TinyChatEngine, compile and run the chat program.</div>
</div><!-- fragment --><p> make chat -j ./chat &lt;model_name&gt; &lt;precision&gt; ```</p>
</li>
</ul>
<h1><a class="anchor" id="autotoc_md5"></a>
Experimental features</h1>
<p>TinyChatEngine offers versatile capabilities suitable for various applications. Additionally, we introduce a sophisticated voice chatbot. Explore our step-by-step guide here to seamlessly deploy a chatbot locally on your device!</p>
<h1><a class="anchor" id="autotoc_md6"></a>
Related Projects</h1>
<p><a href="https://github.com/mit-han-lab/tinyengine">TinyEngine</a></p>
<p><a href="https://github.com/mit-han-lab/smoothquant">Smoothquant</a></p>
<p><a href="https://github.com/mit-han-lab/llm-awq">AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</a></p>
<h1><a class="anchor" id="autotoc_md7"></a>
Acknowledgement</h1>
<p><a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a></p>
<p><a href="https://github.com/ggerganov/whisper.cpp">whisper.cpp</a></p>
<p><a href="https://github.com/huggingface/transformers">transformers</a> </p>
</div></div><!-- PageDoc -->
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.7
</small></address>
</body>
</html>
