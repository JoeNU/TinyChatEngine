<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.7"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>TinyChatEngine: TinyChatEngine</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">TinyChatEngine
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.7 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

<div><div class="header">
  <div class="headertitle"><div class="title">TinyChatEngine </div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="md_README"></a> <img src="tinychat_logo.png" alt="" class="inline" title="tinychat_logo"/>    </p>
<h1><a class="anchor" id="autotoc_md0"></a>
TinyChatEngine: On-Device LLM Inference Library</h1>
<p>TinyChatEngine is a powerful LLM inference library specifically designed for the efficient deployment of quantized large language models (LLMs) on edge devices.</p>
<div class="image">
<img src="chat_demo.gif" alt=""/>
<div class="caption">
chat_demo</div></div>
    <p>Feel free to check out our <a href="assets/slides.pdf">slides</a> for more details!</p>
<h2><a class="anchor" id="autotoc_md1"></a>
Prerequisites</h2>
<h3><a class="anchor" id="autotoc_md2"></a>
MacOS</h3>
<p>For MacOS, install boost and llvm by</p>
<div class="fragment"><div class="line">brew install boost</div>
<div class="line">brew install llvm</div>
</div><!-- fragment --><p>For M1/M2 users, install Xcode from AppStore to enable the metal compiler for GPU support.</p>
<h3><a class="anchor" id="autotoc_md3"></a>
Windows</h3>
<p>For Windows, download and install the GCC compiler with MSYS2. Follow this tutorial: <a href="https://code.visualstudio.com/docs/cpp/config-mingw">https://code.visualstudio.com/docs/cpp/config-mingw</a> for installation.</p>
<ul>
<li>Install required dependencies with MSYS2</li>
</ul>
<div class="fragment"><div class="line">pacman -S --needed base-devel mingw-w64-x86_64-toolchain make unzip git</div>
</div><!-- fragment --><ul>
<li>Add binary directories (e.g., C:\msys64\mingw64\bin and C:\msys64\usr\bin) to the environment path</li>
</ul>
<h2><a class="anchor" id="autotoc_md4"></a>
Step-by-step to deploy LLaMA2-7B-chat with TinyChatEngine</h2>
<p>Here, we provide step-by-step instructions to deploy LLaMA2-7B-chat with TinyChatEngine from scratch.</p>
<ul>
<li>Download the repo. <div class="fragment"><div class="line">git clone --recursive https://github.com/mit-han-lab/TinyChatEngine</div>
</div><!-- fragment --></li>
<li>Download the quantized LLaMA2-7B-chat model from our model zoo. <div class="fragment"><div class="line">cd TinyChatEngine/llm</div>
</div><!-- fragment --><ul>
<li>On an x86 device (e.g., Intel/AMD laptop) <div class="fragment"><div class="line">python tools/download_model.py --model LLaMA2_7B_chat_awq_int4 --QM QM_x86</div>
</div><!-- fragment --></li>
<li>On an ARM device (e.g., M1/M2 Macbook, Raspberry Pi) <div class="fragment"><div class="line">python tools/download_model.py --model LLaMA2_7B_chat_awq_int4 --QM QM_ARM</div>
</div><!-- fragment --></li>
<li>On a CUDA device (e.g., Jetson AGX Orin, PC/Server) <code>bash python tools/download_model.py --model LLaMA2_7B_chat_awq_int4 --QM QM_CUDA </code></li>
<li>Check this table for the detailed list of supported models</li>
</ul>
</li>
<li>Compile and start the chat locally. <div class="fragment"><div class="line">make chat -j</div>
<div class="line">./chat</div>
<div class="line">Using model: LLaMA7B_2_chat</div>
<div class="line">Using LLaMA&#39;s default data format: INT4</div>
<div class="line">Loading model... Finished!</div>
<div class="line">USER: Write a syllabus for Operating Systems.</div>
<div class="line">ASSISTANT:</div>
<div class="line">Of course! Here is a sample syllabus for a college-level course on operating systems:</div>
<div class="line">Course Title: Introduction to Operating Systems</div>
<div class="line">Course Description: This course provides an overview of the fundamental concepts and techniques used in modern operating systems, including process management, memory management, file systems, security, and I/O devices. Students will learn how these components work together to provide a platform for running applications and programs on a computer.</div>
<div class="line">Course Objectives:</div>
<div class="line">* Understand the basic architecture of an operating system</div>
<div class="line">* Learn about processes, threads, and process scheduling algorithms</div>
<div class="line">* Study memory management techniques such as paging and segmentation</div>
<div class="line">* Explore file systems including file organization, storage devices, and file access methods</div>
<div class="line">* Investigate security mechanisms to protect against malicious software attacks</div>
<div class="line">* Analyze input/output (I/O) operations and their handling by the operating system</div>
<div class="line">...</div>
</div><!-- fragment --></li>
</ul>
<h2><a class="anchor" id="autotoc_md5"></a>
Backend support</h2>
<p>| Precision | x86 (Intel/AMD CPU) | ARM (Apple M1/M2) | Nvidia GPU | Apple GPU | | ---&mdash; | ------------------------&mdash; | ------&mdash; | ------&mdash; | ------&mdash; | | FP32 | ✅ | ✅ | | | FP16 | | | | | W4A16 | | | ✅ | ✅ | W4A32 | ✅ | ✅ | | ✅ | W4A8 | ✅ | ✅ | | | W8A8 | ✅ | ✅ | |</p>
<h2><a class="anchor" id="autotoc_md6"></a>
Quantization and Model Support</h2>
<p>The goal of TinyChatEngine is to support various quantization methods on various devices. For example, At present, it supports the quantized weights for int8 opt models that originate from <a href="https://github.com/mit-han-lab/smoothquant">smoothquant</a> using the provided conversion script <a href="llm/opt_smooth_exporter.py">opt_smooth_exporter.py</a>. For LLaMA models, scripts are available for converting Huggingface format checkpoints to our int4 wegiht <a href="llm/llama_exporter.py">format</a>, and for quantizing them to specific methods <a href="llm/model_quantizer.py">based on your device</a>. Before converting and quantizing your models, it is recommended to apply the fake quantization from <a href="https://github.com/mit-han-lab/llm-awq">AWQ</a> to achieve better accuracy. We are currently working on supporting more models, please stay tuned!</p>
<h3><a class="anchor" id="autotoc_md7"></a>
Device-specific int4 Weight Reordering</h3>
<p>To mitigate the runtime overheads associated with weight reordering, TinyChatEngine conducts this process offline during model conversion. In this section, we will explore the weight layouts of QM_ARM and QM_x86. These layouts are tailored for ARM and x86 CPUs, supporting 128-bit SIMD and 256-bit SIMD operations, respectively. We also support QM_CUDA for Nvidia GPUs, including server and edge GPUs.</p>
<table class="markdownTable">
<tr class="markdownTableHead">
<th class="markdownTableHeadNone">Platforms   </th><th class="markdownTableHeadNone">ISA   </th><th class="markdownTableHeadNone">Quantization methods    </th></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Intel/AMD   </td><td class="markdownTableBodyNone">x86-64   </td><td class="markdownTableBodyNone">QM_x86    </td></tr>
<tr class="markdownTableRowEven">
<td class="markdownTableBodyNone">Apple M1/M2 Mac   </td><td class="markdownTableBodyNone">arm   </td><td class="markdownTableBodyNone">QM_ARM    </td></tr>
<tr class="markdownTableRowOdd">
<td class="markdownTableBodyNone">Nvidia GPU   </td><td class="markdownTableBodyNone">CUDA   </td><td class="markdownTableBodyNone">QM_CUDA   </td></tr>
</table>
<ul>
<li>Example layout of QM_ARM: For QM_ARM, consider the initial configuration of a 128-bit weight vector, [w0, w1, ... , w30, w31], where each wi is a 4-bit quantized weight. TinyChatEngine rearranges these weights in the sequence [w0, w16, w1, w17, ..., w15, w31] by interleaving the lower half and upper half of the weights. This new arrangement facilitates the decoding of both the lower and upper halves using 128-bit AND and shift operations, as depicted in the subsequent figure. This will eliminate runtime reordering overheads and improve performance.</li>
</ul>
<h2><a class="anchor" id="autotoc_md8"></a>
Download and deploy models from our Model Zoo</h2>
<p>We offer a selection of models that have been tested with TinyChatEngine. These models can be readily downloaded and deployed on your device. To download a model, locate the target model's ID in the table below and use the associated script.</p>
<table class="doxtable">
<tr>
<th>Models </th><th>Precisions </th><th>ID </th><th>x86 backend </th><th>ARM backend </th><th>CUDA backend    </th></tr>
<tr>
<td rowspan="2">LLaMA2_13B_chat </td><td>fp32 </td><td>LLaMA2_13B_chat_fp32  </td><td>✅  </td><td>✅  </td><td></td></tr>
<tr>
<td>int4 </td><td>LLaMA2_13B_chat_awq_int4 </td><td>✅  </td><td>✅  </td><td>✅   </td></tr>
<tr>
<td rowspan="2">LLaMA2_7B_chat </td><td>fp32 </td><td>LLaMA2_7B_chat_fp32  </td><td>✅  </td><td>✅  </td><td></td></tr>
<tr>
<td>int4 </td><td>LLaMA2_7B_chat_awq_int4 </td><td>✅  </td><td>✅  </td><td>✅   </td></tr>
<tr>
<td rowspan="2">LLaMA_7B </td><td>fp32 </td><td>LLaMA_7B_fp32  </td><td>✅  </td><td>✅  </td><td></td></tr>
<tr>
<td>int4 </td><td>LLaMA_7B_awq_int4 </td><td>✅  </td><td>✅  </td><td>✅   </td></tr>
<tr>
<td rowspan="3">opt-6.7B </td><td>fp32 </td><td>opt_6.7B_fp32 </td><td>✅  </td><td>✅  </td><td></td></tr>
<tr>
<td>int8 </td><td>opt_6.7B_smooth_int8 </td><td>✅  </td><td>✅  </td><td></td></tr>
<tr>
<td>int4 </td><td>opt_6.7B_awq_int4 </td><td>✅  </td><td>✅  </td><td></td></tr>
<tr>
<td rowspan="3">opt-1.3B </td><td>fp32 </td><td>opt_1.3B_fp32 </td><td>✅  </td><td>✅  </td><td></td></tr>
<tr>
<td>int8 </td><td>opt_1.3B_smooth_int8 </td><td>✅  </td><td>✅  </td><td></td></tr>
<tr>
<td>int4 </td><td>opt_1.3B_awq_int4 </td><td>✅  </td><td>✅  </td><td></td></tr>
<tr>
<td rowspan="3">opt-125m </td><td>fp32 </td><td>opt_125m_fp32 </td><td>✅  </td><td>✅  </td><td></td></tr>
<tr>
<td>int8 </td><td>opt_125m_smooth_int8 </td><td>✅  </td><td>✅  </td><td></td></tr>
<tr>
<td>int4 </td><td>opt_125m_awq_int4 </td><td>✅  </td><td>✅  </td><td></td></tr>
</table>
<p>For instance, to download the quantized LLaMA-2-7B-chat model: (for int4 models, use &ndash;QM to choose the quantized model for your device)</p>
<ul>
<li>On an Intel/AMD latptop: <div class="fragment"><div class="line">python tools/download_model.py --model LLaMA2_7B_chat_awq_int4 --QM QM_x86</div>
</div><!-- fragment --></li>
<li>On an M1/M2 Macbook: <div class="fragment"><div class="line">python tools/download_model.py --model LLaMA2_7B_chat_awq_int4 --QM QM_ARM</div>
</div><!-- fragment --></li>
<li>On an Nvidia GPU: <div class="fragment"><div class="line">python tools/download_model.py --model LLaMA2_7B_chat_awq_int4 --QM QM_CUDA</div>
</div><!-- fragment --></li>
</ul>
<p>To deploy a quantized model with TinyChatEngine, compile and run the chat program.</p>
<div class="fragment"><div class="line">make chat -j</div>
<div class="line">./chat &lt;model_name&gt; &lt;precision&gt;</div>
</div><!-- fragment --><h2><a class="anchor" id="autotoc_md9"></a>
Experimental features</h2>
<p>TinyChatEngine offers versatile capabilities suitable for various applications. Additionally, we introduce a sophisticated voice chatbot. Explore our step-by-step guide here to seamlessly deploy a chatbot locally on your device!</p>
<h2><a class="anchor" id="autotoc_md10"></a>
Related Projects</h2>
<p><a href="https://github.com/mit-han-lab/tinyengine">TinyEngine</a></p>
<p><a href="https://github.com/mit-han-lab/smoothquant">Smoothquant</a></p>
<p><a href="https://github.com/mit-han-lab/llm-awq">AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</a></p>
<h2><a class="anchor" id="autotoc_md11"></a>
Acknowledgement</h2>
<p><a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a></p>
<p><a href="https://github.com/ggerganov/whisper.cpp">whisper.cpp</a></p>
<p><a href="https://github.com/huggingface/transformers">transformers</a> </p>
</div></div><!-- PageDoc -->
</div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.7
</small></address>
</body>
</html>
